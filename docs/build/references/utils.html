

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Utils &mdash; PySigma  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Example Models" href="../examples/index.html" />
    <link rel="prev" title="Other Structural Nodes" href="graphical/other-structural-nodes.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> PySigma
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Table of Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction/intro.html">Introduction and Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../introduction/getting-started.html">Getting Started</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API References</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="cognitive.html">Cognitive Language Interface</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cognitive.html#type">Type</a></li>
<li class="toctree-l3"><a class="reference internal" href="cognitive.html#predicate">Predicate</a></li>
<li class="toctree-l3"><a class="reference internal" href="cognitive.html#conditional">Conditional</a></li>
<li class="toctree-l3"><a class="reference internal" href="cognitive.html#variablemap">VariableMap</a></li>
<li class="toctree-l3"><a class="reference internal" href="cognitive.html#factorfunction">FactorFunction</a></li>
<li class="toctree-l3"><a class="reference internal" href="cognitive.html#summarization">Summarization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="graphical/index.html">Graphical Architecture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="graphical/data-structures.html">Basic Data Structures</a><ul>
<li class="toctree-l4"><a class="reference internal" href="graphical/data-structures.html#variable-metatypes">Variable Metatypes</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/data-structures.html#variable">Variable</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/data-structures.html#message-types">Message Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/data-structures.html#message">Message</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/data-structures.html#linkdata">LinkData</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="graphical/abstract-base-nodes.html">Abstract Base Nodes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="graphical/abstract-base-nodes.html#node">Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/abstract-base-nodes.html#factor-node">Factor Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/abstract-base-nodes.html#variable-node">Variable Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/abstract-base-nodes.html#alpha-factor-node">Alpha Factor Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/abstract-base-nodes.html#beta-factor-node">Beta Factor Node</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="graphical/predicate-nodes.html">Predicate Subgraph Nodes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="graphical/predicate-nodes.html#working-memory-variable-node">Working Memory Variable Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/predicate-nodes.html#long-term-memory-factor-node">Long-Term Memory Factor Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/predicate-nodes.html#parameter-store-factor-node">Parameter Store Factor Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/predicate-nodes.html#perceptual-buffer-node">Perceptual Buffer Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/predicate-nodes.html#working-memory-factor-node">Working Memory Factor Node</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="graphical/conditional-nodes.html">Conditional Subgraph Nodes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="graphical/conditional-nodes.html#expansion-summarization-factor-node">Expansion / Summarization Factor Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/conditional-nodes.html#relational-mapping-factor-node">Relational Mapping Factor Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/conditional-nodes.html#concatenation-marginalization-transformation-factor-node">Concatenation, Marginalization, &amp; Transformation Factor Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/conditional-nodes.html#filter-variable-node">Filter Variable Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/conditional-nodes.html#event-aggregation-factor-node">Event Aggregation Factor Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/conditional-nodes.html#event-resolution-factor-node">Event Resolution Factor Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/conditional-nodes.html#beta-join-factor-node">Beta Join Factor Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/conditional-nodes.html#gamma-factor-node">Gamma Factor Node</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="graphical/other-structural-nodes.html">Other Structural Nodes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="graphical/other-structural-nodes.html#default-factor-node">Default Factor Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="graphical/other-structural-nodes.html#default-variable-node">Default Variable Node</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Utils</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#utility-functions">Utility Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distributionserver">DistributionServer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#knowledgeserver">KnowledgeServer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Example Models</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PySigma</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">API References</a> &raquo;</li>
        
      <li>Utils</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/references/utils.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="utils">
<h1>Utils<a class="headerlink" href="#utils" title="Permalink to this headline">¶</a></h1>
<div class="section" id="utility-functions">
<h2>Utility Functions<a class="headerlink" href="#utility-functions" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="pysigma.utils.compatible_shape">
<code class="sig-prename descclassname">pysigma.utils.</code><code class="sig-name descname">compatible_shape</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">msg_shape1</span></em>, <em class="sig-param"><span class="n">msg_shape2</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#compatible_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.compatible_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks whether the two given message shapes are compatible.</p>
<p>Both <cite>msg_shape1</cite> and <cite>msg_shape2</cite> should be an iterable of <cite>torch.Size</cite> and have the contents
<code class="docutils literal notranslate"><span class="pre">(batch_shape,</span> <span class="pre">param_shape,</span> <span class="pre">sample_shape,</span> <span class="pre">event_shape)</span></code>. An empty shape, i.e., <code class="docutils literal notranslate"><span class="pre">torch.Size([])</span></code>, will be deemed
compatible with any other shape. <cite>msg_shape1</cite> is compatible with <cite>msg_shape2</cite> if and only if all of its four entries
are compatible with their counterpart in <cite>msg_shape2</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>msg_shape1</strong> (<em>tuple of torch.Size</em>) – First shape. Should have the format <code class="docutils literal notranslate"><span class="pre">(batch_shape,</span> <span class="pre">param_shape,</span> <span class="pre">sample_shape,</span> <span class="pre">event_shape)</span></code>.</p></li>
<li><p><strong>msg_shape2</strong> (<em>tuple of torch.Size</em>) – Second shape. Same as <cite>msg_shape1</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if both shape are compatible.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="distributionserver">
<h2>DistributionServer<a class="headerlink" href="#distributionserver" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="pysigma.utils.DistributionServer">
<em class="property">class </em><code class="sig-prename descclassname">pysigma.utils.</code><code class="sig-name descname">DistributionServer</code><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer" title="Permalink to this definition">¶</a></dt>
<dd><p>Serving distribution class dependent utilities</p>
<ul>
<li><p>Conversion between PyTorch distribution parameters and distribution instance:</p>
<p><code class="docutils literal notranslate"><span class="pre">param2dist()</span></code>, <code class="docutils literal notranslate"><span class="pre">dist2param()</span></code></p>
</li>
<li><p>Translation between PyTorch distribution parameters and natural parameters for exponential family distribution:</p>
<p><code class="docutils literal notranslate"><span class="pre">natural2exp_dist()</span></code>, <code class="docutils literal notranslate"><span class="pre">exp_dist2natural()</span></code></p>
</li>
<li><p>Get vector of moments from a given distribution instance:</p>
<p><code class="docutils literal notranslate"><span class="pre">get_moments()</span></code></p>
</li>
<li><p>Draw particles from distribution instance:</p>
<p><code class="docutils literal notranslate"><span class="pre">draw_particles()</span></code></p>
</li>
<li><p>Get log probability density from given particles:</p>
<p><code class="docutils literal notranslate"><span class="pre">log_pdf()</span></code></p>
</li>
</ul>
<p>Certain distribution classes require special handling, for example for those categorized as finite discrete,
particle values will be drawn uniformly, covering every value in the RV’s value range (support) once and only once,
while assigning each particle its probability mass as its particle weight.</p>
<p>Therefore we delegate all such special handling to this class on an individual basis.</p>
<p>Note that input and output will conform to the format understandable by PyTorch’s distribution class. To
translate to and from formats compatible to PySigma’s predicate knowledge, use KnowledgeServer class</p>
<dl class="py method">
<dt id="pysigma.utils.DistributionServer.param2dist">
<em class="property">classmethod </em><code class="sig-name descname">param2dist</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dist_class</span></em>, <em class="sig-param"><span class="n">param</span></em>, <em class="sig-param"><span class="n">b_shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">e_shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">dist_info</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer.param2dist"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer.param2dist" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts distribution parameter to a distribution instance.</p>
<p>Depending on the context and Predicate knowledge format, the parameter <cite>param</cite> may belong to different
representation systems, in which case it should be interpreted differently. Such specification should be
sufficiently described in the argument <cite>dist_info</cite> in a prior consent format.</p>
<p>The optional arguments <cite>b_shape</cite> and <cite>e_shape</cite> stand for distribution’s batch shape and event shape
respectively. They are used primarily for sanity check. Note that this event shape <cite>e_shape</cite> pertains to
PyTorch’s distribution class specification, and therefore may or may not be different from the event shape of
particles in PySigma’s cognitive format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dist_class</strong> (<em>type</em>) – The distribution class. Must be a subclass of <code class="docutils literal notranslate"><span class="pre">torch.distributions.Distribution</span></code>.</p></li>
<li><p><strong>param</strong> (<em>torch.Tensor</em>) – The parameter tensor. The last dimension is assumed to be the parameter dimension, and sizes of the
dimensions at the front should be equal to <cite>b_shape</cite>.</p></li>
<li><p><strong>b_shape</strong> (<em>torch.Size</em><em>, </em><em>optional</em>) – The batch shape of the distribution. Used for shape sanity check.</p></li>
<li><p><strong>e_shape</strong> (<em>torch.Size</em><em>, </em><em>optional</em>) – The presumed event shape of the distribution. Used for shape sanity check.</p></li>
<li><p><strong>dist_info</strong> (<em>dict</em><em>, </em><em>optional</em>) – An optional dict containing all relevant information in order to correctly interpret the parameter <cite>param</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The instantiated distribution instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.distributions.Distribution</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>NotImplementedError</strong> – If the conversion procedure specific to <cite>dist_class</cite> has not been implemented yet.</p></li>
<li><p><strong>ValueError</strong> – If the converted distribution instance has different batch shape and event shape than specified <cite>b_shape</cite>
    and <cite>e_shape</cite> respectively.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Implemented distribution classes:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 29%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head" rowspan="2"><p>Distribution class</p></th>
<th class="head" rowspan="2"><p>Input parameter shape</p></th>
<th class="head" colspan="2"><p>Returned distribution shapes</p></th>
<th class="head"><p>Notes</p></th>
</tr>
<tr class="row-even"><th class="head"><p>dist.batch_shape</p></th>
<th class="head"><p>dist.event_shape</p></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><p>Categorical</p></td>
<td><p>b_shape + [ num_logits ]</p></td>
<td><p>b_shape</p></td>
<td><p>[] (empty shape)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Normal</p></td>
<td><p>b_shape + [ 2 ]</p></td>
<td><p>b_shape</p></td>
<td><p>[] (empty shape)</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.DistributionServer.dist2param">
<em class="property">classmethod </em><code class="sig-name descname">dist2param</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dist</span></em>, <em class="sig-param"><span class="n">dist_info</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer.dist2param"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer.dist2param" title="Permalink to this definition">¶</a></dt>
<dd><p>Extract the parameter tensor from a given distribution instance.</p>
<p>Depending on the context and Predicate knowledge format, the desired parameter may belong to different
representation systems, in which case it should be generated differently. Such specification should be
sufficiently described in the argument <cite>dist_info</cite> in a prior consent format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dist</strong> (<em>torch.distributions.Distribution</em>) – The distribution instance from which to extract the parameter.</p></li>
<li><p><strong>dist_info</strong> (<em>dict</em><em>, </em><em>optional</em>) – An optional dict containing all relevant information in order to correctly generate the parameter <cite>param</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The parameter tensor in the desired format.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – If the conversion procedure specific to the distribution class of <cite>dist</cite> has not been implemented yet.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Implemented distribution classes:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 32%" />
<col style="width: 56%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Distribution class</p></th>
<th class="head"><p>Returned parameter shapes</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Categorical</p></td>
<td><p>dist.batch_shape + [ num_logits ]</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Normal</p></td>
<td><p>dist.batch_shape + [ 2 ]</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.DistributionServer.get_moments">
<em class="property">classmethod </em><code class="sig-name descname">get_moments</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dist</span></em>, <em class="sig-param"><span class="n">n_moments</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer.get_moments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer.get_moments" title="Permalink to this definition">¶</a></dt>
<dd><p>Get vector of moments from a given distribution instance</p>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Implement with dist_info</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.DistributionServer.draw_particles">
<em class="property">classmethod </em><code class="sig-name descname">draw_particles</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dist</span></em>, <em class="sig-param"><span class="n">num_particles</span></em>, <em class="sig-param"><span class="n">dist_info</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer.draw_particles"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer.draw_particles" title="Permalink to this definition">¶</a></dt>
<dd><p>Draw a list of <cite>num_particles</cite> event particles from the given distribution specified by <cite>dist</cite>. The event
particles drawn will be in the format compatible with DistributionServer and PyTorch.</p>
<p>Note that, some PyTorch distributions have empty event shape. In these cases, the returned particle tensors will
have a singleton event dimension appended at the end.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dist</strong> (<em>torch.distributions.Distribution</em>) – The distribution instance from which to sample particles</p></li>
<li><p><strong>num_particles</strong> (<em>int</em>) – The number of particles to be drawn</p></li>
<li><p><strong>dist_info</strong> (<em>dict</em><em>, </em><em>optional</em>) – Additional dist info necessary for drawing particles in the correct format.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the list of particles drawn, of shape <code class="docutils literal notranslate"><span class="pre">[num_particles]</span> <span class="pre">+</span> <span class="pre">event_shape</span></code>. <code class="docutils literal notranslate"><span class="pre">event_shape</span></code> will at least be
a singleton shape <code class="docutils literal notranslate"><span class="pre">Size([1])</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – If the given distribution yields multi-dimensional events, and no corresponding special drawing method is
    found in <code class="docutils literal notranslate"><span class="pre">cls.dict_draw_particles</span></code> method map.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Unless distribution-class-specific drawing method is specified and registered in <code class="docutils literal notranslate"><span class="pre">cls.dict_draw_particles</span></code>
method map, the distribution instance <cite>dist</cite> will be directly queried to draw the list of samples. Nonetheless,
the default process is only applicable to distributions with a single event dimensions. For those that have
multiple event dimensions, e.g. some that generates matrix-shaped samples, special method has to be implemented.</p>
<p>The distribution instance <cite>dist</cite> is assumed batched, with a variable batch size(shape). However, we want to draw
a single unique list of particles that is representative of each and every single distribution in the batch,
i.e., draw a list of particles from the joint distribution regardless of the batch dimensions. Therefore, we
take the view that drawing samples from <cite>dist</cite> simultaneously across the batch, which results in a sample tensor
that involves the batch dimension, and ignoring the batch dimensions, is equivalent to first selecting uniformly
which single distribution in the batch we wish draw from, and drawing samples from it, and repeating this
process over and over again. The latter approach, when the samples are aggregated, yields a particle list
that is representative of the joint distribution of the whole batch.</p>
<p>Mathematically, we would like to draw particles <code class="docutils literal notranslate"><span class="pre">p</span></code> from the joint distribution:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">~</span> <span class="n">P</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span> <span class="o">=</span> <span class="n">P</span><span class="p">(</span><span class="n">V</span> <span class="o">|</span> <span class="n">B</span><span class="p">)</span> <span class="o">*</span> <span class="n">P</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">V</span></code> is the random variable, and <code class="docutils literal notranslate"><span class="pre">B</span></code> is the batch index. In this way, <code class="docutils literal notranslate"><span class="pre">p</span></code> will be representative of
each and every distribution instance stored in <code class="docutils literal notranslate"><span class="pre">dist</span></code>, which can be expressed as <code class="docutils literal notranslate"><span class="pre">P(V</span> <span class="pre">|</span> <span class="pre">B)</span></code>. Note that
the expression above effectively describes a Markov chain. Therefore, <code class="docutils literal notranslate"><span class="pre">p</span></code> can be drawn in steps:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">~</span> <span class="n">P</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="n">p</span> <span class="o">~</span> <span class="n">P</span><span class="p">(</span><span class="n">V</span> <span class="o">|</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<p>However, since we assume <code class="docutils literal notranslate"><span class="pre">P(B)</span></code> is uniform, this process is equivalent to iteratively drawing <code class="docutils literal notranslate"><span class="pre">p</span></code> from
<code class="docutils literal notranslate"><span class="pre">P(V</span> <span class="pre">|</span> <span class="pre">b)</span></code> for each <code class="docutils literal notranslate"><span class="pre">b</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">p_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
   <span class="n">p</span> <span class="o">~</span> <span class="n">P</span><span class="p">(</span><span class="n">V</span> <span class="o">|</span> <span class="n">b</span><span class="p">)</span>
   <span class="n">p_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>
</div>
<p>Conveniently, the default batch sampling method implemented in PyTorch distribution class already provide us
the above method to generate a batch of particles uniformly. Therefore, the sampling process is implemented by
first drawing <cite>n</cite> batched samples from <cite>dist</cite>, where
<code class="docutils literal notranslate"><span class="pre">n</span> <span class="pre">=</span> <span class="pre">num_particles</span> <span class="pre">//</span> <span class="pre">batch_size</span> <span class="pre">+</span> <span class="pre">1</span></code>, then collapsing the batch dimensions, and finally randomly shuffling
across the collapsed sample dimension, and truncate to select only a number of <code class="docutils literal notranslate"><span class="pre">num_particles</span></code> samples.</p>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.DistributionServer.log_prob">
<em class="property">classmethod </em><code class="sig-name descname">log_prob</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dist</span></em>, <em class="sig-param"><span class="n">values</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer.log_prob"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer.log_prob" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the log probability mass/density of the given particle values w.r.t. the given batched distribution
instance.</p>
<p><cite>values</cite> should have at least 2 dimensions. Its last dimension will be interpreted as the event dimension, and
every other dimensions at front be interpreted as sample dimensions. The size of its event dimension, i.e.,
event shape, must be compatible with the event shape of the distribution instance, i.e., <code class="docutils literal notranslate"><span class="pre">dist.event_shape</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p>If <code class="docutils literal notranslate"><span class="pre">dist.event_shape</span> <span class="pre">==</span> <span class="pre">torch.Size([])</span></code>, i.e., empty shape, the last dimension of <cite>values</cite> must be
singleton, i.e., <code class="docutils literal notranslate"><span class="pre">values.shape[-1]</span> <span class="pre">==</span> <span class="pre">1</span></code>. This is because PyTorch distribution class by default does not
retain the singleton event dimension if it is empty, while in PySigma’s representation of particles, a
separate event dimension must present.</p></li>
<li><p>Otherwise if <code class="docutils literal notranslate"><span class="pre">dist.event_shape</span></code> is not an empty shape, then it must be that
<code class="docutils literal notranslate"><span class="pre">values.event_shape[-1:]</span> <span class="pre">==</span> <span class="pre">dist.event_shape</span></code>.</p></li>
</ul>
</div></blockquote>
<p>An AssertionError will be thrown if the above check does not pass. Accordingly, if <code class="docutils literal notranslate"><span class="pre">values.shape[-1]</span> <span class="pre">==</span> <span class="pre">1</span></code>,
then <code class="docutils literal notranslate"><span class="pre">values</span></code>’s last dimension will be squeezed before being queried against the distribution instance <cite>dist</cite>.</p>
<p>The distribution instance <cite>dist</cite> is assumed batched. In other words, its batch shape <code class="docutils literal notranslate"><span class="pre">dist.batch_shape</span></code> should
not be empty.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dist</strong> (<em>torch.distributions.Distribution</em>) – A batched distribution instance. Its batch shape <code class="docutils literal notranslate"><span class="pre">dist.batch_shape</span></code> should not be empty.</p></li>
<li><p><strong>values</strong> (<em>torch.Tensor</em>) – A tensor with shape <code class="docutils literal notranslate"><span class="pre">(sample_shape</span> <span class="pre">+</span> <span class="pre">[event_size])</span></code>. The last dimension must present.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The log probability mass/density tensor, of shape <code class="docutils literal notranslate"><span class="pre">(dist.batch_shape</span> <span class="pre">+</span> <span class="pre">sample_shape)</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>AssertionError</strong> – If <cite>dist</cite> has empty batch shape.</p></li>
<li><p><strong>AssertionError</strong> – If the <code class="docutils literal notranslate"><span class="pre">event_size</span></code> found in <cite>values</cite> is not compatible with <cite>dist.event_shape</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.DistributionServer.kl_norm">
<em class="property">classmethod </em><code class="sig-name descname">kl_norm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dist1</span></em>, <em class="sig-param"><span class="n">dist2</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer.kl_norm"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer.kl_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the norm of the KL divergence of two given batched distributions</p>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.DistributionServer.transform_param">
<em class="property">classmethod </em><code class="sig-name descname">transform_param</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">param</span></em>, <em class="sig-param"><span class="n">dist_info</span></em>, <em class="sig-param"><span class="n">trans</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer.transform_param"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer.transform_param" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition-todo admonition" id="id2">
<p class="admonition-title">Todo</p>
<p>To implement</p>
</div>
<p>Return the parameter of the transformed distribution</p>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.DistributionServer._default_get_moments">
<em class="property">classmethod </em><code class="sig-name descname">_default_get_moments</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dist</span></em>, <em class="sig-param"><span class="n">n_moments</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer._default_get_moments"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer._default_get_moments" title="Permalink to this definition">¶</a></dt>
<dd><p>Default method for getting moments, but only supports up to second order moment (i.e. X^2)</p>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.DistributionServer._categorical_param2dist">
<em class="property">static </em><code class="sig-name descname">_categorical_param2dist</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span></em>, <em class="sig-param"><span class="n">dist_info</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer._categorical_param2dist"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer._categorical_param2dist" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition-todo admonition" id="id3">
<p class="admonition-title">Todo</p>
<p>TODO: different parameter scheme and dist_info schema specification</p>
</div>
<p>For Categorical distribution, params assumed by default to be the values of ‘probs’ attribute</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>torch.Tensor</em>) – By default, of shape <code class="docutils literal notranslate"><span class="pre">batch_shape</span> <span class="pre">+</span> <span class="pre">[num_logits]</span></code>, where <code class="docutils literal notranslate"><span class="pre">num_logits</span></code> is the number of possible
outcomes of the categorical random variable.</p></li>
<li><p><strong>dist_info</strong> (<em>dict</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns a categorical distribution instance, with <code class="docutils literal notranslate"><span class="pre">batch_shape</span></code> the same as the batch shape of input
<cite>param</cite>, and <code class="docutils literal notranslate"><span class="pre">event_shape</span> <span class="pre">==</span> <span class="pre">torch.Size([])</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.distributions.Categorical</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#categorical">torch.distributions.Categorical</a></p>
</div>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.DistributionServer._categorical_dist2param">
<em class="property">static </em><code class="sig-name descname">_categorical_dist2param</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dist</span></em>, <em class="sig-param"><span class="n">dist_info</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer._categorical_dist2param"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer._categorical_dist2param" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition-todo admonition" id="id4">
<p class="admonition-title">Todo</p>
<p>TODO: different parameter scheme and dist_info schema specification</p>
</div>
<p>For Categorical distribution, params assumed by default to be the values of ‘probs’ attribute</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dist</strong> (<em>torch.distributions.Categorical</em>) – </p></li>
<li><p><strong>dist_info</strong> (<em>dict</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>By default, returns <cite>dist.probs</cite>, of shape <code class="docutils literal notranslate"><span class="pre">dist.batch_shape</span> <span class="pre">+</span> <span class="pre">[num_logits]</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#categorical">torch.distributions.Categorical</a></p>
</div>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.DistributionServer._normal_param2dist">
<em class="property">static </em><code class="sig-name descname">_normal_param2dist</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">params</span></em>, <em class="sig-param"><span class="n">dist_info</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer._normal_param2dist"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer._normal_param2dist" title="Permalink to this definition">¶</a></dt>
<dd><p>For Univariate Normal distributions, <code class="docutils literal notranslate"><span class="pre">param_shape</span></code> is <code class="docutils literal notranslate"><span class="pre">[2]</span></code>. The first slice of the input parameter
tensor will be interpreted as the <cite>loc</cite> argument to the PyTorch Normal distribution, and the second slice
will be interpreted as the <cite>scale</cite> argument.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>torch.Tensor</em>) – Parameter tensor, with <code class="docutils literal notranslate"><span class="pre">param_shape</span> <span class="pre">==</span> <span class="pre">Size([2])</span></code>, i.e., last dimension has a size of 2.</p></li>
<li><p><strong>dist_info</strong> (<em>dict</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A univariate normal distribution instance</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.distributions.Normal</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#normal">torch.distributions.Normal</a></p>
</div>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.DistributionServer._normal_dist2param">
<em class="property">static </em><code class="sig-name descname">_normal_dist2param</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dist</span></em>, <em class="sig-param"><span class="n">dist_info</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#DistributionServer._normal_dist2param"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.DistributionServer._normal_dist2param" title="Permalink to this definition">¶</a></dt>
<dd><p>For Univariate Normal distribution, the returned parameter tensor will be <code class="docutils literal notranslate"><span class="pre">dist.loc</span></code> and <code class="docutils literal notranslate"><span class="pre">dist.scale</span></code>
stacked together along a new event dimension appended to as the last dimension. The first slice will be
<code class="docutils literal notranslate"><span class="pre">dist.loc</span></code> and the second slice be <code class="docutils literal notranslate"><span class="pre">dist.scale</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dist</strong> (<em>torch.distributions.Normal</em>) – </p></li>
<li><p><strong>dist_info</strong> (<em>dict</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The parameter tensor, with the last event dimension have a size of 2</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference external" href="https://pytorch.org/docs/stable/distributions.html#normal">torch.distributions.Normal</a></p>
</div>
</dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.DistributionServer.dict_draw_particles">
<code class="sig-name descname">dict_draw_particles</code><em class="property"> = {}</em><a class="headerlink" href="#pysigma.utils.DistributionServer.dict_draw_particles" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.DistributionServer.dict_log_pdf">
<code class="sig-name descname">dict_log_pdf</code><em class="property"> = {}</em><a class="headerlink" href="#pysigma.utils.DistributionServer.dict_log_pdf" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.DistributionServer.dict_param2dist">
<code class="sig-name descname">dict_param2dist</code><em class="property"> = {&lt;class 'torch.distributions.categorical.Categorical'&gt;: &lt;staticmethod object&gt;, &lt;class 'torch.distributions.normal.Normal'&gt;: &lt;staticmethod object&gt;}</em><a class="headerlink" href="#pysigma.utils.DistributionServer.dict_param2dist" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.DistributionServer.dict_dist2param">
<code class="sig-name descname">dict_dist2param</code><em class="property"> = {&lt;class 'torch.distributions.categorical.Categorical'&gt;: &lt;staticmethod object&gt;, &lt;class 'torch.distributions.normal.Normal'&gt;: &lt;staticmethod object&gt;}</em><a class="headerlink" href="#pysigma.utils.DistributionServer.dict_dist2param" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.DistributionServer.dict_natural2exp_param">
<code class="sig-name descname">dict_natural2exp_param</code><em class="property"> = {}</em><a class="headerlink" href="#pysigma.utils.DistributionServer.dict_natural2exp_param" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.DistributionServer.dict_exp_param2natural">
<code class="sig-name descname">dict_exp_param2natural</code><em class="property"> = {}</em><a class="headerlink" href="#pysigma.utils.DistributionServer.dict_exp_param2natural" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.DistributionServer.dict_natural2exp_dist">
<code class="sig-name descname">dict_natural2exp_dist</code><em class="property"> = {}</em><a class="headerlink" href="#pysigma.utils.DistributionServer.dict_natural2exp_dist" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.DistributionServer.dict_exp_dist2natural">
<code class="sig-name descname">dict_exp_dist2natural</code><em class="property"> = {}</em><a class="headerlink" href="#pysigma.utils.DistributionServer.dict_exp_dist2natural" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.DistributionServer.dict_get_moments">
<code class="sig-name descname">dict_get_moments</code><em class="property"> = {}</em><a class="headerlink" href="#pysigma.utils.DistributionServer.dict_get_moments" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="knowledgeserver">
<h2>KnowledgeServer<a class="headerlink" href="#knowledgeserver" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="pysigma.utils.KnowledgeServer">
<em class="property">class </em><code class="sig-prename descclassname">pysigma.utils.</code><code class="sig-name descname">KnowledgeServer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dist_class</span></em>, <em class="sig-param"><span class="n">rv_sizes</span></em>, <em class="sig-param"><span class="n">rv_constraints</span></em>, <em class="sig-param"><span class="n">rv_num_particles</span><span class="o">=</span><span class="default_value">[]</span></em>, <em class="sig-param"><span class="n">dist_info</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer" title="Permalink to this definition">¶</a></dt>
<dd><p>Knowledge Server class. Provides service regarding a Predicate’s knowledge.</p>
<p>The architecture should hold one KnowledgeServer instance for each Predicate instantiated to cache knowledge
contents and provide distribution related service.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dist_class</strong> (<em>type</em>) – The distribution class of the Predicate’s knowledge. Must be a subclass of <code class="docutils literal notranslate"><span class="pre">torch.distributions.Distribution</span></code>.</p></li>
<li><p><strong>rv_sizes</strong> (<em>iterable of int</em>) – The sizes of the random variables of the Predicate’s knowledge. Note that the order given by the iterable will
be respected.</p></li>
<li><p><strong>rv_constraints</strong> (<em>iterable of torch.distributions.constraints.Constraint</em>) – The value constraints of the random variables. Note that the order given by the iterable will be respected.</p></li>
<li><p><strong>rv_num_particles</strong> (<em>iterable of int</em>) – The number of marginal particles that should be drawn w.r.t. each random variable. Must have the same length as
<cite>rv_sizes</cite> and <cite>rv_constraints</cite>, i.e., the number of random variables.</p></li>
<li><p><strong>dist_info</strong> (<em>dict</em><em>, </em><em>optional</em>) – An optional attribute dict that contains all necessary information for DistributionServer to draw particles and
query particles’ log pdf.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="pysigma.utils.KnowledgeServer.dist_class">
<code class="sig-name descname">dist_class</code><a class="headerlink" href="#pysigma.utils.KnowledgeServer.dist_class" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>type</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.KnowledgeServer.rv_sizes">
<code class="sig-name descname">rv_sizes</code><a class="headerlink" href="#pysigma.utils.KnowledgeServer.rv_sizes" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>tuple of int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.KnowledgeServer.rv_constraints">
<code class="sig-name descname">rv_constraints</code><a class="headerlink" href="#pysigma.utils.KnowledgeServer.rv_constraints" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>tuple of torch.distributions.constraints.Constraint</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.KnowledgeServer.rv_num_particles">
<code class="sig-name descname">rv_num_particles</code><a class="headerlink" href="#pysigma.utils.KnowledgeServer.rv_num_particles" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>tuple of int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.KnowledgeServer.dist_info">
<code class="sig-name descname">dist_info</code><a class="headerlink" href="#pysigma.utils.KnowledgeServer.dist_info" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.KnowledgeServer.num_rvs">
<code class="sig-name descname">num_rvs</code><a class="headerlink" href="#pysigma.utils.KnowledgeServer.num_rvs" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of random variables involved in specifying the Predicate knowledge.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.KnowledgeServer.e_shape">
<code class="sig-name descname">e_shape</code><a class="headerlink" href="#pysigma.utils.KnowledgeServer.e_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>The event shape of predicate’s knowledge. Inferred form <cite>rv_sizes</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>torch.Size</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.KnowledgeServer.particles">
<code class="sig-name descname">particles</code><a class="headerlink" href="#pysigma.utils.KnowledgeServer.particles" title="Permalink to this definition">¶</a></dt>
<dd><p>The cached tuple of marginal particle event tensors corresponding to the random variables. This attribute is set
when <cite>draw_grid_particles</cite> is called with <code class="docutils literal notranslate"><span class="pre">update_cache=True</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>tuple of torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt id="pysigma.utils.KnowledgeServer.log_densities">
<code class="sig-name descname">log_densities</code><a class="headerlink" href="#pysigma.utils.KnowledgeServer.log_densities" title="Permalink to this definition">¶</a></dt>
<dd><p>The cached tuple of log sampling density tensors corresponding to each of the marginal particle event. This
attribute is set when <cite>draw_grid_particles</cite> is called with <code class="docutils literal notranslate"><span class="pre">update_cache=True</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>tuple of torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>In order to provide service to both predicate nodes and conditional nodes in all stages, KnowledgeServer should
store and manipulate data regarding the random variables only. In other words, only message components that do not
involve batch dimensions should be cached; this includes particle value tensors and log sampling density tensors,
but excludes both parameter and weight tensors. The latter ones’ shapes are not invariant throughout the stages
in the conditional subgraph, and therefore should be specified by the callee.</p>
<p>Signatures for special private distribution class dependent methods:</p>
<ul class="simple">
<li><p>Cognitive to PyTorch event format translation method: <code class="docutils literal notranslate"><span class="pre">2torch_event(particles)</span> <span class="pre">--&gt;</span> <span class="pre">particles</span></code></p></li>
<li><p>PyTorch to Cognitive event format translation method: <code class="docutils literal notranslate"><span class="pre">2cognitive_event(particles)</span> <span class="pre">--&gt;</span> <span class="pre">particles</span></code></p></li>
<li><p>Special marginal particle list sampling method: <code class="docutils literal notranslate"><span class="pre">special_draw(batched_dist)</span> <span class="pre">--&gt;</span> <span class="pre">particles,</span> <span class="pre">log_densities</span></code></p></li>
</ul>
<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer.draw_particles">
<code class="sig-name descname">draw_particles</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batched_param</span></em>, <em class="sig-param"><span class="n">batch_shape</span></em>, <em class="sig-param"><span class="n">update_cache</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer.draw_particles"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer.draw_particles" title="Permalink to this definition">¶</a></dt>
<dd><p>Draws new particles for the associated predicate w.r.t. the given <cite>batched_param</cite>. Returns necessary
components to instantiate a particles message.</p>
<p>This method is typically called by the predicate’s LTMFN node during modification phase, in which the new
updated batched parameter tensor has been obtained and provided by <cite>batched_param</cite>. This method is then
proceed to:</p>
<ol class="arabic simple">
<li><p>instantiate the batched distribution instances from the batched parameter tensor,</p></li>
<li><p>draw a single unique list of <strong>marginal</strong> particle values w.r.t. each random variable from the entire batch
of distribution instances,</p></li>
<li><p>calculate their corresponding marginal sampling densities,</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batched_param</strong> (<em>torch.Tensor</em>) – The new batched parameter tensor, of shape <code class="docutils literal notranslate"><span class="pre">(batch_shape</span> <span class="pre">+</span> <span class="pre">[param_size])</span></code>.</p></li>
<li><p><strong>batch_shape</strong> (<em>torch.Size</em>) – The batch shape</p></li>
<li><p><strong>update_cache</strong> (<em>bool</em>) – Whether to replace the cache content in <code class="docutils literal notranslate"><span class="pre">self.particles</span></code> and <code class="docutils literal notranslate"><span class="pre">self.log_densities</span></code> with the result of
calling this method.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>particles</strong> (<em>tuple of torch.Tensor</em>) – The marginal particle lists w.r.t. each random variable in order. The i-th particle tensor has shape
<code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">s_shape[i],</span> <span class="pre">e_shape[i]</span> <span class="pre">]</span></code>.</p></li>
<li><p><strong>log_densities</strong> (<em>tuple of torch.Tensor</em>) – The marginal sampling log densities w.r.t. each random variable in order. The i-th log sampling density
tensor has shape <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">s_shape[i]</span> <span class="pre">]</span></code>.</p></li>
</ul>
</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Some remarks regarding the aforementioned step 2 and 3:</p>
<p>The tuple set of the types of the rv constraints specified in <code class="docutils literal notranslate"><span class="pre">self.rv_constraints</span></code> will be used to look up
the pre-specified method map <code class="docutils literal notranslate"><span class="pre">self.dict_2special_draw</span></code>. If an entry present, will used that method to obtain
the returning <code class="docutils literal notranslate"><span class="pre">particles</span></code> and <code class="docutils literal notranslate"><span class="pre">log_densities</span></code>. This is particularly useful, for instance, in the case of
finite discrete random variables where a regular lattice should be drawn uniformly.</p>
<p>Otherwise, the standard procedure will be carried out.
``</p>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer.surrogate_log_prob">
<code class="sig-name descname">surrogate_log_prob</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">param</span></em>, <em class="sig-param"><span class="n">alt_particles</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">index_map</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer.surrogate_log_prob"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer.surrogate_log_prob" title="Permalink to this definition">¶</a></dt>
<dd><p>Query the log pdf of the surrogate particles specified by <cite>alt_particles</cite> w.r.t. the cached distribution
instance.</p>
<p>A batched distribution instance will be instantiated from <cite>param</cite>, along with registered metadata in
<cite>self.dist_info</cite>.</p>
<p>If <cite>index_map</cite> is not specified, each entries in the iterable <cite>alt_particles</cite> must represent events of the
Predicate’s random argument at the same index in the predicate argument list. If an entry in <cite>alt_particles</cite> is
‘None’, then the respective cached particle tensor in <cite>self.particles</cite> representing that predicate argument will
be used instead.</p>
<p>Alternatively, <cite>index_map</cite> can be specified as a dictionary mapping integer index to an integer index or a list
of indices. The entry <code class="docutils literal notranslate"><span class="pre">alt_particles[i]</span></code> will be taken as the particle tensor for the <code class="docutils literal notranslate"><span class="pre">index_map[i]</span></code> th
predicate argument. If <code class="docutils literal notranslate"><span class="pre">index_map[i]</span></code> is a list of integers, then the particle tensor <code class="docutils literal notranslate"><span class="pre">alt_particles[i]</span></code>
will be interpreted as the <strong>concatenated/joint events</strong> of the corresponding predicate arguments.</p>
<p>For example, if <code class="docutils literal notranslate"><span class="pre">index_map[i]</span> <span class="pre">=</span> <span class="pre">[0,</span> <span class="pre">3]</span></code>, then the particle tensor <code class="docutils literal notranslate"><span class="pre">alt_particles[i]</span></code> will be regarded as
the joint events of the 0-th and 3-rd predicate arguments. This means that an attempt will be made to
combinatorially de-concatenate the tensor <code class="docutils literal notranslate"><span class="pre">alt_particles[i]</span></code>. If this process fails, an AssertionError will
be thrown.</p>
<p>Note that the entry <code class="docutils literal notranslate"><span class="pre">alt_particles[i]</span></code> can be <code class="docutils literal notranslate"><span class="pre">None</span></code>, however in this case <code class="docutils literal notranslate"><span class="pre">index_map[i]</span></code> must refer to
one predicate argument only, i.e., <code class="docutils literal notranslate"><span class="pre">index_map[i]</span></code> must be an integer. <strong>If there is any predicate argument
that is not referenced by values of</strong> <cite>index_map</cite> <strong>, then the returning</strong> <cite>surrogate_log_prob</cite> <strong>will be
marginalized over this predicate argument.</strong></p>
<p>Accordingly, if <code class="docutils literal notranslate"><span class="pre">index_map</span></code> is specified, then all indices of <code class="docutils literal notranslate"><span class="pre">alt_particles</span></code> must appear as keys in
<code class="docutils literal notranslate"><span class="pre">index_map</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – The alternative parameter from which a surrogate distribution instance is to be instantiated and log prob
being queried.</p></li>
<li><p><strong>alt_particles</strong> (<em>list of</em><em> (</em><em>torch.Tensor</em><em> or </em><em>None</em><em>)</em><em>, or </em><em>None</em>) – The surrogate particles to be queried. If not None, each entry must either be None, so that the
corresponding cached articles will be used instead, or a torch.Tensor, with a shape of length 2 and the last
dimension size equal to the corresponding value in <code class="docutils literal notranslate"><span class="pre">self.rv_sizes</span></code>. Must specify if <cite>index_map</cite> is
specified. Defaults to None.</p></li>
<li><p><strong>index_map</strong> (<em>dict</em><em>, or </em><em>None</em>) – The optional index mapping. If specified, all applicable indices into <cite>alt_particles</cite> must appear as keys
in this dict. The <code class="docutils literal notranslate"><span class="pre">i</span></code> th entry in <cite>alt_particles</cite> will be taken as the surrogate particles for the
predicate argument whose index is <code class="docutils literal notranslate"><span class="pre">index_map[i]</span></code> if <code class="docutils literal notranslate"><span class="pre">index_map[i]</span></code> is an integer, or the joint surrogate
particles for those arguments whose indices appear in <code class="docutils literal notranslate"><span class="pre">index_map[i]</span></code> if <code class="docutils literal notranslate"><span class="pre">index_map[i]</span></code> is a list.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The log probability tensor, of shape <code class="docutils literal notranslate"><span class="pre">(batch_shape</span> <span class="pre">+</span> <span class="pre">sample_shape)</span></code>, where <code class="docutils literal notranslate"><span class="pre">batch_shape</span></code> is the batch
shape of the Predicate’s knowledge, inferred from the shape of <cite>param</cite>, and <code class="docutils literal notranslate"><span class="pre">sample_shape</span></code> is the list of
sample sizes of the queried particles <strong>in the order given by `alt_particles`</strong>. In other words,
<code class="docutils literal notranslate"><span class="pre">sample_shape[i]</span> <span class="pre">==</span> <span class="pre">alt_particles[i].shape[0]</span></code>. If <code class="docutils literal notranslate"><span class="pre">alt_particles[i]</span></code> is None, then it is the sample
size of the cached particle tensor respectively.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AssertionError</strong> – If <cite>alt_particles</cite> contains <code class="docutils literal notranslate"><span class="pre">None</span></code> but <code class="docutils literal notranslate"><span class="pre">self.particles</span></code> is also None, meaning no cached particles.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer.event2torch_event">
<code class="sig-name descname">event2torch_event</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">cat_particles</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer.event2torch_event"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer.event2torch_event" title="Permalink to this definition">¶</a></dt>
<dd><p>Translates joint particle event values from the Cognitive format to a format understandable by PyTorch
distribution class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>cat_particles</strong> (<em>torch.Tensor</em>) – A tensor representing the list of concatenated particle events in Cognitive format. Its last dimension will
be taken as the event dimension and should be equal to the sum of rv sizes in <code class="docutils literal notranslate"><span class="pre">self.rv_sizes</span></code>, while all
other dimensions will be taken as the sample dimensions.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor representing a list of translated particle events from <cite>cat_particles</cite>. Its last dimension size
depends on the PyTorch format representation of events, while the sizes of other dimensions are the same as
<cite>cat_particles</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The specific translation method may vary depending on the distribution class. Therefore, this method serves only
as an API entry point where the specific translation procedure will be looked up in <code class="docutils literal notranslate"><span class="pre">self.dict_2torch_event</span></code>
using the registered distribution class <code class="docutils literal notranslate"><span class="pre">self.dist_class</span></code>. If no entry is found, then will assume no special
translation is necessary and will return the input <cite>cat_particles</cite> as is.</p>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer.event2cognitive_event">
<code class="sig-name descname">event2cognitive_event</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">particles</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer.event2cognitive_event"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer.event2cognitive_event" title="Permalink to this definition">¶</a></dt>
<dd><p>Translates joint particle event values from the PyTorch distribution class format to Cognitive format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>particles</strong> (<em>torch.Tensor</em>) – A tensor representing the particle events in PyTorch-compatible format. Its last dimension will be taken
as the event dimension, while all other dimensions will be taken as the sample dimensions.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A concatenated tensor representing a list of translated particle events from <cite>cat_particles</cite>, where the
events are concatenated along the last dimension, with size of each chunk in accordance with
<cite>self.rv_sizes</cite>, and the sizes of all other dimensions are the same as <cite>particles</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The specific translation method may vary depending on the distribution class. Therefore, this method serves only
as an API entry point where the specific translation procedure will be looked up in
<code class="docutils literal notranslate"><span class="pre">self.dict_2cognitive_event</span></code> using the registered distribution class <code class="docutils literal notranslate"><span class="pre">self.dist_class</span></code>. If no entry is
found, then will assume no special translation is necessary and will return the input <cite>cat_particles</cite> as is.</p>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer.combinatorial_cat">
<em class="property">static </em><code class="sig-name descname">combinatorial_cat</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">particles</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer.combinatorial_cat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer.combinatorial_cat" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper static method that combinatorially concatenates the list of event particles specified by <cite>particles</cite>.</p>
<p>Returns the contained tensor directly if there is only one entry in <cite>particles</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>particles</strong> (<em>iterable of torch.Tensor</em>) – The list of particles to be concatenated. Each element should be a tensor with a shape of length 2, where
the first dimension is assumed the sample dimension, and second dimension assumed the event dimension.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>The combinatorially concatenated event particle tensor of shape:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">sample_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">...</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="n">event_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+...+</span><span class="n">event_size</span><span class="p">[</span><span class="n">m</span><span class="p">]]</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">sample_size[i]</span></code> is the sample size of the <code class="docutils literal notranslate"><span class="pre">i</span></code> th particle, and similarly is <code class="docutils literal notranslate"><span class="pre">event_size[i]</span></code>.
Its total number of dimensions, i.e. <code class="docutils literal notranslate"><span class="pre">.dim()</span></code>, is equal to the number of random variables plus 1.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer.combinatorial_decat">
<em class="property">static </em><code class="sig-name descname">combinatorial_decat</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">cat_particles</span></em>, <em class="sig-param"><span class="n">split_sizes</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer.combinatorial_decat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer.combinatorial_decat" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper static method that combinatorially de-concatenate the joint particles specified by <cite>cat_particles</cite>,
with the event size of the particle tensors in each de-concatenated list given by <cite>split_sizes</cite>. This method
implements the exact opposite operation of <cite>combinatorial_cat</cite>.</p>
<p>An exception will be raised if <cite>cat_particles</cite> cannot be properly de-concatenated, for instance if it is not
previously a result produced by <cite>combinatorial_cat</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cat_particles</strong> (<em>torch.Tensor</em>) – <p>The concatenated particle tensor, of shape:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">sample_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">...</span><span class="p">,</span> <span class="n">sample_size</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="nb">sum</span><span class="p">(</span><span class="n">split_sizes</span><span class="p">)]</span>
</pre></div>
</div>
<p>where <cite>m</cite> is the number of variables to split/de-concatenate, to which the length of <cite>split_sizes</cite> should
equal.</p>
</p></li>
<li><p><strong>split_sizes</strong> (<em>list of int</em>) – A list of integers denoting the event size of each split variable in order.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The tuple of de-concatenated particles. The <cite>i</cite> th entry has shape <code class="docutils literal notranslate"><span class="pre">[sample_size[i],</span> <span class="pre">split_sizes[i]]</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple of torch.Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – If <cite>cat_particles</cite> was not a result from <cite>combinatorial_cat</cite> and cannot be properly de-concatenated.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer._default_draw">
<code class="sig-name descname">_default_draw</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batched_dist</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer._default_draw"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer._default_draw" title="Permalink to this definition">¶</a></dt>
<dd><p>The process for drawing marginal particles and calculating corresponding sampling densities:</p>
<p>Assume there are two random variables <cite>V</cite> and <cite>U</cite>. We would like to draw samples <code class="docutils literal notranslate"><span class="pre">v</span></code> and <code class="docutils literal notranslate"><span class="pre">u</span></code> from the
joint distribution</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">v</span> <span class="o">~</span> <span class="n">P</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="n">u</span> <span class="o">~</span> <span class="n">P</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
<p>where <cite>B</cite> is the batch index. Using <cite>DistributionServer.draw_particles()</cite>, we can already obtain a list of
joint particles <code class="docutils literal notranslate"><span class="pre">(v_i,</span> <span class="pre">u_i)</span></code> from <code class="docutils literal notranslate"><span class="pre">P(V,</span> <span class="pre">U,</span> <span class="pre">B)</span></code>. Refer to the documentation of that method for more
details.</p>
<p>Now, we take apart <code class="docutils literal notranslate"><span class="pre">v</span></code> and <code class="docutils literal notranslate"><span class="pre">u</span></code> from the joint particles list, and form a lattice of re-assembled particles
<code class="docutils literal notranslate"><span class="pre">(v_i,</span> <span class="pre">u_j)</span></code>. The individual values <code class="docutils literal notranslate"><span class="pre">v_i</span></code> and <code class="docutils literal notranslate"><span class="pre">u_j</span></code> can be then taken as the final marginal particles, and
it is straightforward to approximate, <strong>up to a constant factor</strong>, the value of their respective sampling
densities . Simply:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">P</span><span class="p">(</span><span class="n">V</span><span class="o">=</span><span class="n">v_i</span><span class="p">)</span> <span class="o">~=</span> <span class="nb">sum</span><span class="p">(</span> <span class="n">P</span><span class="p">(</span><span class="n">V</span><span class="o">=</span><span class="n">v_i</span><span class="p">,</span> <span class="n">U</span><span class="o">=</span><span class="n">u_j</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="n">b</span><span class="p">)</span> <span class="p">)</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">u_j</span> <span class="ow">and</span> <span class="n">b</span>
<span class="n">P</span><span class="p">(</span><span class="n">U</span><span class="o">=</span><span class="n">u_j</span><span class="p">)</span> <span class="o">~=</span> <span class="nb">sum</span><span class="p">(</span> <span class="n">P</span><span class="p">(</span><span class="n">V</span><span class="o">=</span><span class="n">v_i</span><span class="p">,</span> <span class="n">U</span><span class="o">=</span><span class="n">u_j</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="n">b</span><span class="p">)</span> <span class="p">)</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">v_i</span> <span class="ow">and</span> <span class="n">b</span>
</pre></div>
</div>
<p>To reiterate, the values calculated herein are not approximations of the actual marginal probabilities, but
approximations that are proportional to the actual marginal probabilities up to a constant factor.</p>
</dd></dl>

<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer._categorical_var_span">
<code class="sig-name descname">_categorical_var_span</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer._categorical_var_span"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer._categorical_var_span" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer._categorical_enforced_sample_shape">
<code class="sig-name descname">_categorical_enforced_sample_shape</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer._categorical_enforced_sample_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer._categorical_enforced_sample_shape" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer._categorical_2torch_event">
<code class="sig-name descname">_categorical_2torch_event</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">particles</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer._categorical_2torch_event"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer._categorical_2torch_event" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer._categorical_2cognitive_event">
<code class="sig-name descname">_categorical_2cognitive_event</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">particles</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer._categorical_2cognitive_event"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer._categorical_2cognitive_event" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="pysigma.utils.KnowledgeServer._categorical_draw">
<code class="sig-name descname">_categorical_draw</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pysigma/utils.html#KnowledgeServer._categorical_draw"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pysigma.utils.KnowledgeServer._categorical_draw" title="Permalink to this definition">¶</a></dt>
<dd><p>Draw particles components for Categorical distributions. Returns a finite discrete well-spaced lattice as the
particle values and uniform log sampling densities.</p>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../examples/index.html" class="btn btn-neutral float-right" title="Example Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="graphical/other-structural-nodes.html" class="btn btn-neutral float-left" title="Other Structural Nodes" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Jincheng Zhou, Yunzhe Wang, Volkan Ustun, Paul Rosenbloom @ USC Institute for Creative Technologies

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>